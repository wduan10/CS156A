{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS156A Homework 5\n",
    "## Wilson Duan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1.\n",
    "$0.008 < (0.1)^2 (1 - \\frac{8+1}{N})$\n",
    "\n",
    "$0.8 < 1 - \\frac{9}{N}$\n",
    "\n",
    "$\\frac{9}{N} < 0.2$\n",
    "\n",
    "$45 < N$\n",
    "\n",
    "The smallest answer choice that is greater than 45 is answer choice **c)** 100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2.\n",
    "For the feature vector $(1, x_1^2, x_2^2)$, the linear classification model is described by $\\text{sign}(w_0 * 1 + w_1x_1^2 + w_2x_2^2)$. According to the hyperbolic decision boundary, we want the model to predict $-1$ for very large $x_1$ and very negative $x_1$. This means that as $x_1^2$ increases, we want the term inside the sign() to be more negative. Therefore, the coefficient to the term $x_1^2$ must be negative, so $w_1 < 0$.\n",
    "\n",
    "We also want to predict $1$ for very large $x_2$ and very negative $x_2$. This means that as $x_2^2$ increases, we want the term inside the sign() to be more positive. Therefore, the coefficient to the term $x_2^2$ must be positive, so $w_2 > 0$. \n",
    "\n",
    "We also know that $w_0$ can be adjusted to accomodate these weights, making the hyperbolic decision boundary possible. From the analysis above, the answer choice we arrive at is **d)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3.\n",
    "The 4th order polynomial transform has a dimension of $d = 14$ (we do not count the 1). The VC dimension of linear models follows the equation $d_{vc} \\leq d + 1 = 14 + 1 = 15$. Therefore, the smallest answer choice that is not smaller than $d_{vc}$ is **c)** 15."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4.\n",
    "$E(u, v) = (ue^v - 2ve^{-u})^2$\n",
    "\n",
    "Using the chain rule,\n",
    "$\\frac{\\partial E(u, v)}{\\partial u} = 2(ue^v - 2ve^{-u})(e^v + 2ve^{-u})$\n",
    "\n",
    "As a result, the answer is **e)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partials(u, v):\n",
    "    partial_u = 2 * (u * np.exp(v) - 2 * v * np.exp(-u)) * \\\n",
    "                (np.exp(v) + 2 * v * np.exp(-u))\n",
    "    partial_v = 2 * (u * np.exp(v) - 2 * v * np.exp(-u)) * \\\n",
    "                (u * np.exp(v) - 2 * np.exp(-u))\n",
    "    return partial_u, partial_v\n",
    "\n",
    "def calculate_error(u, v):\n",
    "    return (u * math.e ** v - 2 * v * math.e ** -u) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations for error to fall below threshold: 10\n"
     ]
    }
   ],
   "source": [
    "error_threshold = 10 ** -14\n",
    "learning_rate = 0.1\n",
    "u, v = 1, 1\n",
    "error = calculate_error(u, v)\n",
    "iterations = 0\n",
    "while (error > error_threshold):\n",
    "    partial_u, partial_v = partials(u, v)\n",
    "    u -= partial_u * learning_rate\n",
    "    v -= partial_v * learning_rate\n",
    "    error = calculate_error(u, v)\n",
    "    iterations += 1\n",
    "\n",
    "print(\"Iterations for error to fall below threshold:\", iterations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the code above, it takes 10 iterations for the error to fall below the $10^{-14}$ threshold, so the answer is **d)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.04473629039778207, 0.023958714099141746)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u, v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the code above, the answer choices that are closest to the u and v found above arre **e)** (0.045, 0.024)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error after 15 full iterations: 0.13981379199615324\n"
     ]
    }
   ],
   "source": [
    "iterations = 15\n",
    "u, v = 1, 1\n",
    "for i in range(iterations):\n",
    "    partial_u, _ = partials(u, v)\n",
    "    u -= partial_u * learning_rate\n",
    "    _, partial_v = partials(u, v)\n",
    "    v -= partial_v * learning_rate\n",
    "\n",
    "    error = calculate_error(u, v)\n",
    "\n",
    "print(\"Error after 15 full iterations:\", error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the code above, the error after 15 full iterations is roughly 0.1398, which is closest to answer choice **a)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a set of helper functions\n",
    "def random_point():\n",
    "    x = random.random() * 2 - 1\n",
    "    y = random.random() * 2 - 1\n",
    "    return (x, y)\n",
    "\n",
    "def random_line():\n",
    "    x1, y1 = random_point()\n",
    "    x2, y2 = random_point()\n",
    "\n",
    "    slope = (y2 - y1) / (x2 - x1)\n",
    "    intercept = y1 - slope * x1\n",
    "    return (slope, intercept)\n",
    "\n",
    "def evaluate_point(slope, intercept, x, y):\n",
    "    if (slope * x + intercept > y):\n",
    "        return -1\n",
    "    return 1\n",
    "\n",
    "def create_dataset(n, slope, intercept):\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(n):\n",
    "        a, b = random_point()\n",
    "        X.append([a, b])\n",
    "        y.append(evaluate_point(slope, intercept, a, b))\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def cross_entropy_error(X, y, w):\n",
    "    return np.mean(np.log(1 + np.exp(-y * np.dot(X, w))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average E_out: 0.10379824890640138\n",
      "Average epochs to converge: 336.44\n"
     ]
    }
   ],
   "source": [
    "N = 100\n",
    "N_test = 1000\n",
    "runs = 100\n",
    "learning_rate = 0.01\n",
    "avg_epochs = 0\n",
    "avg_error = 0\n",
    "\n",
    "for i in range(runs):\n",
    "    # generate train and test datasets\n",
    "    slope, intercept = random_line()\n",
    "    X_train, y_train = create_dataset(N, slope, intercept)\n",
    "    train_set = list(zip(X_train, y_train))\n",
    "    X_test, y_test = create_dataset(N_test, slope, intercept)\n",
    "\n",
    "    # initialize\n",
    "    w = np.zeros(3)\n",
    "    epochs = 0\n",
    "\n",
    "    # train model\n",
    "    while True:\n",
    "        w_prev = np.copy(w)\n",
    "        # shuffle data\n",
    "        random.shuffle(train_set)\n",
    "        for x, y in train_set:\n",
    "            x = np.insert(x, 0, 1)\n",
    "            z = np.dot(w, x)\n",
    "            gradient = (-y * x) / (1 + np.exp(y * z))\n",
    "            w -= learning_rate * gradient\n",
    "\n",
    "        epochs += 1\n",
    "        if (np.linalg.norm(w - w_prev) < 0.01):\n",
    "            break\n",
    "        \n",
    "\n",
    "    # test model\n",
    "    X_test_modified = np.zeros((N_test, 3))\n",
    "    for j in range(len(X_test)):\n",
    "        X_test_modified[j] = np.insert(X_test[j], 0, 1)\n",
    "    \n",
    "    error = cross_entropy_error(X_test_modified, y_test, w)\n",
    "    avg_error += error\n",
    "    avg_epochs += epochs\n",
    "\n",
    "avg_error /= runs\n",
    "avg_epochs /= runs\n",
    "print(\"Average E_out:\", avg_error)\n",
    "print(\"Average epochs to converge:\", avg_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the code above, the average E_out is closest to answer choice **d)** 0.100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 9.\n",
    "According to the code above, the average epochs it takes to converge is closest to answer choice **a)** 350."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using SGD, the weights are updated point by point. When a point x is classified properly, we don't want to update the weights, and we want the error to be zero. This makes answer choices a, b, and d incorrect because they update the weights no matter the classification accuracy. When a point is misclassified, we want the error to be $-yw^Tx$ in order to simulate a PLA, so the answer is **e)**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
