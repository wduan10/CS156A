{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS156A Homework 6\n",
    "## Wilson Duan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since H' is a subset of H, the size of H' is smaller than H, meaning H' represents a smaller set of hypotheses than H. As a result, H' is less representative of the target function f, meaning deterministic noise will increase in general. Thus, the answer is **b)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_data = []\n",
    "out_data = []\n",
    "\n",
    "with open(\"in.dta\", \"r\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip().split()\n",
    "        line = [float(x) for x in line]\n",
    "        in_data.append(line)\n",
    "\n",
    "with open(\"out.dta\", \"r\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip().split()\n",
    "        line = [float(x) for x in line]\n",
    "        out_data.append(line)\n",
    "\n",
    "in_data = np.array(in_data)\n",
    "out_data = np.array(out_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(data):\n",
    "    output = np.zeros((len(data), 8))\n",
    "    for i in range(len(data)):\n",
    "        x1, x2 = data[i]\n",
    "        output[i] = [1, x1, x2, x1**2, x2**2, x1*x2, abs(x1-x2), abs(x1+x2)]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression(X, y):\n",
    "    inversed = np.linalg.inv(X.transpose().dot(X))\n",
    "    w = inversed.dot(X.transpose()).dot(y)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression_decay(X, y, lmbda):\n",
    "    inversed = np.linalg.inv(X.transpose().dot(X) + lmbda * np.eye(X.shape[1]))\n",
    "    w = inversed.dot(X.transpose()).dot(y)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_error(X, y, w):\n",
    "    predictions = np.sign(X.dot(w))\n",
    "    return np.mean(predictions != y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In sample error: 0.02857142857142857\n",
      "Out of sample error: 0.084\n"
     ]
    }
   ],
   "source": [
    "X_in = transform_data(in_data[:, :2])\n",
    "y_in = in_data[:, 2]\n",
    "X_out = transform_data(out_data[:, :2])\n",
    "y_out = out_data[:, 2]\n",
    "\n",
    "w = linear_regression(X_in, y_in)\n",
    "print(\"In sample error:\", calculate_error(X_in, y_in, w))\n",
    "print(\"Out of sample error:\", calculate_error(X_out, y_out, w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the code output above, the in sample and out of sample classification errors are closest to answer choice **a)** 0.03, 0.08."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In sample error: 0.02857142857142857\n",
      "Out of sample error: 0.08\n"
     ]
    }
   ],
   "source": [
    "k = -3\n",
    "lmbda = 10 ** k\n",
    "\n",
    "w = linear_regression_decay(X_in, y_in, lmbda)\n",
    "print(\"In sample error:\", calculate_error(X_in, y_in, w))\n",
    "print(\"Out of sample error:\", calculate_error(X_out, y_out, w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the code output above, the in sample and out of sample classification errors are closest to answer choice **d)** 0.03, 0.08."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In sample error: 0.37142857142857144\n",
      "Out of sample error: 0.436\n"
     ]
    }
   ],
   "source": [
    "k = 3\n",
    "lmbda = 10 ** k\n",
    "\n",
    "w = linear_regression_decay(X_in, y_in, lmbda)\n",
    "print(\"In sample error:\", calculate_error(X_in, y_in, w))\n",
    "print(\"Out of sample error:\", calculate_error(X_out, y_out, w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the code output above, the in sample and out of sample classification errors are closest to answer choice **e)** 0.4, 0.4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for k =  2\n",
      "In sample error: 0.2\n",
      "Out of sample error: 0.228\n",
      "\n",
      "Results for k =  1\n",
      "In sample error: 0.05714285714285714\n",
      "Out of sample error: 0.124\n",
      "\n",
      "Results for k =  0\n",
      "In sample error: 0.0\n",
      "Out of sample error: 0.092\n",
      "\n",
      "Results for k =  -1\n",
      "In sample error: 0.02857142857142857\n",
      "Out of sample error: 0.056\n",
      "\n",
      "Results for k =  -2\n",
      "In sample error: 0.02857142857142857\n",
      "Out of sample error: 0.084\n",
      "\n"
     ]
    }
   ],
   "source": [
    "k_vals = [2, 1, 0, -1, -2]\n",
    "\n",
    "for k in k_vals:\n",
    "    lmbda = 10 ** k\n",
    "\n",
    "    w = linear_regression_decay(X_in, y_in, lmbda)\n",
    "    print(\"Results for k = \", k)\n",
    "    print(\"In sample error:\", calculate_error(X_in, y_in, w))\n",
    "    print(\"Out of sample error:\", calculate_error(X_out, y_out, w))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the code output above, the k that achieves the smallest out of sample classification error is answer choice **d)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for k =  2\n",
      "In sample error: 0.2\n",
      "Out of sample error: 0.228\n",
      "\n",
      "Results for k =  1\n",
      "In sample error: 0.05714285714285714\n",
      "Out of sample error: 0.124\n",
      "\n",
      "Results for k =  0\n",
      "In sample error: 0.0\n",
      "Out of sample error: 0.092\n",
      "\n",
      "Results for k =  -1\n",
      "In sample error: 0.02857142857142857\n",
      "Out of sample error: 0.056\n",
      "\n",
      "Results for k =  -2\n",
      "In sample error: 0.02857142857142857\n",
      "Out of sample error: 0.084\n",
      "\n",
      "Results for k =  -3\n",
      "In sample error: 0.02857142857142857\n",
      "Out of sample error: 0.08\n",
      "\n",
      "Results for k =  -4\n",
      "In sample error: 0.02857142857142857\n",
      "Out of sample error: 0.084\n",
      "\n"
     ]
    }
   ],
   "source": [
    "k_vals = [2, 1, 0, -1, -2, -3, -4]\n",
    "\n",
    "for k in k_vals:\n",
    "    lmbda = 10 ** k\n",
    "\n",
    "    w = linear_regression_decay(X_in, y_in, lmbda)\n",
    "    print(\"Results for k = \", k)\n",
    "    print(\"In sample error:\", calculate_error(X_in, y_in, w))\n",
    "    print(\"Out of sample error:\", calculate_error(X_out, y_out, w))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the code output above, the smallest out of sample classification error is closest to answer choice **b)** 0.06."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In all of the answer choices, the hypothesis sets used are $H(10, 0, 3)$, $H(10, 0, 4)$, $H(10, 1, 3)$, and $H(10, 1, 4)$. We first analyze each hypothesis set.\n",
    "\n",
    "$H(10, 0, 3)$ represents the set of all hypotheses in $H_{10}$ such that all the weights for degree greater than or equal to 3 are zero. As a result, $H(10, 0, 3)$ only spans all polynomials with degree 2 or less. In other words, $H(10, 0, 3) = H_2$.\n",
    "\n",
    "$H(10, 0, 4)$ represents the set of all hypotheses in $H_{10}$ such that all the weights for degree greater than or equal to 4 are zero. As a result, $H(10, 0, 4)$ only spans all polynomials with degree 3 or less. In other words, $H(10, 0, 4) = H_3$.\n",
    "\n",
    "$H(10, 1, 3)$ represents the set of all hypotheses in $H_{10}$ such that all the weights for degree greater than or equal to 3 are 1. This means $H(10, 1, 3)$ contains all polynomials with degree 2 or less, as well as some polynomials with degree up to 10. In other words, $H_2 \\subset H(10, 1, 3)$.\n",
    "\n",
    "$H(10, 1, 4)$ represents the set of all hypotheses in $H_{10}$ such that all the weights for degree greater than or equal to 4 are 1. This means $H(10, 1, 4)$ contains all polynomials with degree 3 or less, as well as some polynomials with degree up to 10. In other words, $H_3 \\subset H(10, 1, 4)$.\n",
    "\n",
    "Now we look at each answer choice.\n",
    "\n",
    "a) $H(10, 0, 3) \\cup H(10, 0, 4)$ equals the union of $H_2$ and $H_3$, which is not equal to $H_4$.\n",
    "\n",
    "b) $H(10, 1, 3) \\cup H(10, 1, 4)$ does not equal $H_3$ because both hypothesis sets contain polynomials with degree up to 10.\n",
    "\n",
    "c) $H(10, 0, 3) \\cap H(10, 0, 4) = H_2 \\cap H_3 = H_2$, so this is the correct answer.\n",
    "\n",
    "d) $H(10, 1, 3) \\cap H(10, 1, 4)$ contains polynomials up to degree 10, so it does not equal $H_1$.\n",
    "\n",
    "The answer is **c)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 8.\n",
    "Since $d^{(0)} = 5$, each of the 3 neurons in the hidden layer has 5 + 1 associated weights, which makes $6 * 3 = 18$ weights in the hidden layer. Since $d^{(1)} = 3$, the neuron in the last layer has 3 + 1 associated weights, which makes for 4 weights in the last layer. As a result, there are 22 weights in this system. Note that the +1 comes from the bias at each neuron.\n",
    "\n",
    "In the forward pass, the operations of the form $w_{ij}^{(l)}x_i^{(l-1)}$ occur a number of times equal to the number of weights, so these operations occur 22 times.\n",
    "\n",
    "The operations of the form $w_{ij}^{(l)}\\delta_j^{(l)}$ occur in the backwards pass:\n",
    "\n",
    "$\\delta_i^{(l-1)} = (1 - (x_i^{(l-1)})^2) \\sum_{j=1}^{d^{(l)}} w_{ij}^{(l)}\\delta_j^{(l)}$\n",
    "\n",
    "For the input layer, there are no operations to account for since there is no signal. For the last layer, there are also no operations to account for since $\\delta$ is known. In the hidden layer, there is only one $i$, and since the summation goes from $j=1$ to $d^{(1)} = 3$, there are 3 operations.\n",
    "\n",
    "The operations of the form $x_i^{(l-1)}\\delta_j^{l}$ occur in the update step, which occurs a number of times equal to the number of weights (since each weight is updated in the update step). Therefore, operations of this form occur 22 times.\n",
    "\n",
    "The total number of operations is $22 + 3 + 22 = 47$. This is closest to answer choice **d)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 9.\n",
    "In order for each layer to be fully connected, there must be $d^{(l)} * (d^{(l + 1)} - 1)$ weights for each l. In order to minimize the sum of these products, we use layers with only two units (one of them is $x_0$). The number of layers becomes $36 / 2 = 18$ hidden layers. From the input layer to the first hidden layer, there are 10 weights because the input layer has dimension 10. From a hidden layer to the next layer, there are 2 weights ($x_0$ and another weight). Going from a hidden layer to another hidden layer occurs 17 times, and going from the last hidden layer to the output unit occurs 1 time, so that makes for $(17 + 1) * 2 = 36$ weights. Adding the number of weights, we get $10 + 36 = 46$ weights. Therefore, the answer is **a)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 10.\n",
    "The maximum possible number of weights occurs when there are 2 hidden layers, as that maximizes the sum of the products $d^{(l)} * (d^{(l + 1)} - 1)$. This is maximized when we have 22 units in the first layer and 14 units in the second layer, giving us a total of $10 * (22 - 1) + 22 * (14 - 1) + 14 * 1 = 510$ weights. Therefore, the answer is **e)**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
