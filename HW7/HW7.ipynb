{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS156A Homework 7\n",
    "## Wilson Duan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_data = []\n",
    "out_data = []\n",
    "\n",
    "with open(\"in.dta\", \"r\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip().split()\n",
    "        line = [float(x) for x in line]\n",
    "        in_data.append(line)\n",
    "\n",
    "with open(\"out.dta\", \"r\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip().split()\n",
    "        line = [float(x) for x in line]\n",
    "        out_data.append(line)\n",
    "\n",
    "in_data = np.array(in_data)\n",
    "out_data = np.array(out_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(data, k):\n",
    "    output = np.zeros((len(data), k + 1))\n",
    "    for i in range(len(data)):\n",
    "        x1, x2 = data[i]\n",
    "        output[i] = [1, x1, x2, x1**2, x2**2, x1*x2, abs(x1-x2), abs(x1+x2)][:k + 1]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression(X, y):\n",
    "    inversed = np.linalg.inv(X.transpose().dot(X))\n",
    "    w = inversed.dot(X.transpose()).dot(y)\n",
    "    return w\n",
    "\n",
    "def calculate_error(X, y, w):\n",
    "    predictions = np.sign(X.dot(w))\n",
    "    return np.mean(predictions != y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(in_data, out_data, training_size, k, invert=False):\n",
    "    X_in = transform_data(in_data[:, :2], k)\n",
    "    y_in = in_data[:, 2]\n",
    "\n",
    "    if (not invert):\n",
    "        X_train, y_train = X_in[:training_size], y_in[:training_size]\n",
    "        X_val, y_val = X_in[training_size:], y_in[training_size:]\n",
    "    else:\n",
    "        X_train, y_train = X_in[-training_size:], y_in[-training_size:]\n",
    "        X_val, y_val = X_in[:-training_size], y_in[:-training_size]\n",
    "\n",
    "    X_test = transform_data(out_data[:, :2], k)\n",
    "    y_test = out_data[:, 2]\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error for k=3: 0.3\n",
      "Out of sample error for k=3: 0.42 \n",
      "\n",
      "Validation error for k=4: 0.5\n",
      "Out of sample error for k=4: 0.416 \n",
      "\n",
      "Validation error for k=5: 0.2\n",
      "Out of sample error for k=5: 0.188 \n",
      "\n",
      "Validation error for k=6: 0.0\n",
      "Out of sample error for k=6: 0.084 \n",
      "\n",
      "Validation error for k=7: 0.1\n",
      "Out of sample error for k=7: 0.072 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_size = 25\n",
    "ks = [3, 4, 5, 6, 7]\n",
    "\n",
    "for k in ks:\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test = split_data(in_data, out_data, training_size, k)\n",
    "    w = linear_regression(X_train, y_train)\n",
    "\n",
    "    # evaluate on validation set\n",
    "    print(f\"Validation error for k={k}:\", calculate_error(X_val, y_val, w))\n",
    "\n",
    "    # evaluate on test set\n",
    "    print(f\"Out of sample error for k={k}:\", calculate_error(X_test, y_test, w), \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the code above, the model with k=6 had the smallest classification error on the validation set, so the answer is **d)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2.\n",
    "According to the code above, the model with k=7 had the smallest out of sample classification error, so the answer is **e)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error for k=3: 0.28\n",
      "Out of sample error for k=3: 0.396 \n",
      "\n",
      "Validation error for k=4: 0.36\n",
      "Out of sample error for k=4: 0.388 \n",
      "\n",
      "Validation error for k=5: 0.2\n",
      "Out of sample error for k=5: 0.284 \n",
      "\n",
      "Validation error for k=6: 0.08\n",
      "Out of sample error for k=6: 0.192 \n",
      "\n",
      "Validation error for k=7: 0.12\n",
      "Out of sample error for k=7: 0.196 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_size = 10\n",
    "ks = [3, 4, 5, 6, 7]\n",
    "\n",
    "for k in ks:\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test = split_data(in_data, out_data, training_size, k, True)\n",
    "    w = linear_regression(X_train, y_train)\n",
    "\n",
    "    # evaluate on validation set\n",
    "    print(f\"Validation error for k={k}:\", calculate_error(X_val, y_val, w))\n",
    "\n",
    "    # evaluate on test set\n",
    "    print(f\"Out of sample error for k={k}:\", calculate_error(X_test, y_test, w), \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the code above, the model with k=6 had the smallest classification error on the validation set, so the answer is **d)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4.\n",
    "According to the code above, the model with k=6 had the smallest out of sample classification error, so the answer is **d)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 5.\n",
    "According to the code above, the answer is closest to **b)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33939114809392207"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 1000\n",
    "min_e = 0\n",
    "for i in range(N):\n",
    "    e1 = random.random()\n",
    "    e2 = random.random()\n",
    "    min_e += min(e1, e2)\n",
    "min_e /= N\n",
    "min_e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expected value of variables following a uniform distribution over [0, 1] is $\\frac{0 + 1}{2} = 0.5$. The expected value of min($e_1$, $e_2$) is around 0.33, as simulated above. Thus, the answer is **d)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def small_transform(data, k):\n",
    "    output = np.zeros((len(data), k + 1))\n",
    "    for i in range(len(data)):\n",
    "        x = data[i]\n",
    "        output[i] = [1, x][:k + 1]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation error for p=2.3941701709713277:\n",
      "h0 error:  0.5\n",
      "h1 error:  1.1350433676859402\n",
      "\n",
      "Cross validation error for p=0.8555996771673521:\n",
      "h0 error:  0.5\n",
      "h1 error:  64.66494840795316\n",
      "\n",
      "Cross validation error for p=4.335661307243996:\n",
      "h0 error:  0.5\n",
      "h1 error:  0.5\n",
      "\n",
      "Cross validation error for p=2.5593964634688433:\n",
      "h0 error:  0.5\n",
      "h1 error:  0.9868839293305474\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "ps = [math.sqrt(math.sqrt(3) + 4), math.sqrt(math.sqrt(3) - 1), math.sqrt(9 + 4 * math.sqrt(6)), math.sqrt(9 - math.sqrt(6))]\n",
    "\n",
    "for p in ps:\n",
    "    data = [[-1, 0], [p, 1], [1, 0]]\n",
    "\n",
    "    h0_error = 0\n",
    "    h1_error = 0\n",
    "    for i in range(len(data)):\n",
    "        train_data = np.array(data[:i] + data[i+1:])\n",
    "        test_data = np.array([data[i]])\n",
    "\n",
    "        X_train, y_train = train_data[:, 0], train_data[:, 1]\n",
    "        X_test, y_test = test_data[:, 0], test_data[:, 1]\n",
    "        \n",
    "        # train and test h0\n",
    "        X_train_0 = small_transform(X_train, 0)\n",
    "        X_test_0 = small_transform(X_test, 0)\n",
    "\n",
    "        w = linear_regression(X_train_0, y_train)\n",
    "        error0 = (X_test_0.dot(w) - y_test) ** 2\n",
    "        h0_error += error0[0]\n",
    "        \n",
    "        # train and test h1\n",
    "        X_train_1 = small_transform(X_train, 1)\n",
    "        X_test_1 = small_transform(X_test, 1)\n",
    "\n",
    "        w = linear_regression(X_train_1, y_train)\n",
    "        error1 = (X_test_1.dot(w) - y_test) ** 2\n",
    "        h1_error += error1[0]\n",
    "\n",
    "    h0_error /= len(data)\n",
    "    h1_error /= len(data)\n",
    "    print(f\"Cross validation error for p={p}:\")\n",
    "    print(\"h0 error: \", h0_error)\n",
    "    print(\"h1 error: \", h1_error)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the code above, when $\\rho = \\sqrt{9 + 4\\sqrt{6}}$, the two models have the same LOOCV squared error, so the answer is **c)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a set of helper functions\n",
    "def random_point():\n",
    "    x = random.random() * 2 - 1\n",
    "    y = random.random() * 2 - 1\n",
    "    return (x, y)\n",
    "\n",
    "def random_line():\n",
    "    x1, y1 = random_point()\n",
    "    x2, y2 = random_point()\n",
    "\n",
    "    slope = (y2 - y1) / (x2 - x1)\n",
    "    intercept = y1 - slope * x1\n",
    "    return (slope, intercept)\n",
    "\n",
    "def evaluate_point(slope, intercept, x, y):\n",
    "    if (slope * x + intercept > y):\n",
    "        return -1\n",
    "    return 1\n",
    "\n",
    "def PLA_predict(weights, x, y):\n",
    "    return np.sign(weights[0] + weights[1] * x + weights[2] * y)\n",
    "\n",
    "def predict(weights, X):\n",
    "    return np.sign(weights[0] + weights[1] * X[:, 0] + weights[2] * X[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(n, slope, intercept):\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(n):\n",
    "        a, b = random_point()\n",
    "        X.append([a, b])\n",
    "        y.append(evaluate_point(slope, intercept, a, b))\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM is better than PLA 60.8% of the time\n"
     ]
    }
   ],
   "source": [
    "num_simulations = 1000\n",
    "N = 10\n",
    "\n",
    "percentage = 0\n",
    "for i in range(num_simulations):\n",
    "    slope, intercept = random_line()\n",
    "    X_train, y_train = create_dataset(N, slope, intercept)\n",
    "    X_test, y_test = create_dataset(1000, slope, intercept)\n",
    "    # in case all points are 1 or -1\n",
    "    while (sum(y_train == np.array([1] * N)) == 0 or sum(y_train == np.array([-1] * N)) == 0):\n",
    "        slope, intercept = random_line()\n",
    "        X_train, y_train = create_dataset(N, slope, intercept)\n",
    "        X_test, y_test = create_dataset(1000, slope, intercept)\n",
    "\n",
    "    weights = np.zeros(3)\n",
    "    # run PLA\n",
    "    while True:\n",
    "        misclassified_points = []\n",
    "        # populate misclassified points\n",
    "        for ((a, b), label) in zip(X_train, y_train):\n",
    "            prediction = PLA_predict(weights, a, b)\n",
    "            if (prediction != label):\n",
    "                misclassified_points.append((a, b, label))\n",
    "\n",
    "        # check for convergence\n",
    "        if (len(misclassified_points) == 0):\n",
    "            break\n",
    "        else:\n",
    "            a, b, label = random.choice(misclassified_points)\n",
    "            weights += label * np.array([1, a, b])\n",
    "\n",
    "    # evaluate PLA performance\n",
    "    pla_accuracy = np.mean(predict(weights, X_test) == y_test)\n",
    "\n",
    "    # train SVM\n",
    "    clf = svm.SVC(C = 10e20, kernel = 'linear')\n",
    "    clf.fit(X_train, y_train)\n",
    "    svm_accuracy = np.mean(np.array(clf.predict(X_test)) == y_test)\n",
    "    \n",
    "    percentage += (int)(svm_accuracy > pla_accuracy)\n",
    "\n",
    "percentage /= num_simulations\n",
    "print(f\"SVM is better than PLA {100 * percentage}% of the time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the code output above, the answer is **c)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM is better than PLA 58.8% of the time\n",
      "Average number of support vectors:  2.996\n"
     ]
    }
   ],
   "source": [
    "num_simulations = 1000\n",
    "N = 100\n",
    "\n",
    "percentage = 0\n",
    "avg_support_vectors = 0\n",
    "for i in range(num_simulations):\n",
    "    slope, intercept = random_line()\n",
    "    X_train, y_train = create_dataset(N, slope, intercept)\n",
    "    X_test, y_test = create_dataset(1000, slope, intercept)\n",
    "    # in case all points are 1 or -1\n",
    "    while (sum(y_train == np.array([1] * N)) == 0 or sum(y_train == np.array([-1] * N)) == 0):\n",
    "        slope, intercept = random_line()\n",
    "        X_train, y_train = create_dataset(N, slope, intercept)\n",
    "        X_test, y_test = create_dataset(1000, slope, intercept)\n",
    "\n",
    "    weights = np.zeros(3)\n",
    "    # run PLA\n",
    "    while True:\n",
    "        misclassified_points = []\n",
    "        # populate misclassified points\n",
    "        for ((a, b), label) in zip(X_train, y_train):\n",
    "            prediction = PLA_predict(weights, a, b)\n",
    "            if (prediction != label):\n",
    "                misclassified_points.append((a, b, label))\n",
    "\n",
    "        # check for convergence\n",
    "        if (len(misclassified_points) == 0):\n",
    "            break\n",
    "        else:\n",
    "            a, b, label = random.choice(misclassified_points)\n",
    "            weights += label * np.array([1, a, b])\n",
    "\n",
    "    # evaluate PLA performance\n",
    "    pla_accuracy = np.mean(predict(weights, X_test) == y_test)\n",
    "\n",
    "    # train SVM\n",
    "    clf = svm.SVC(C = 10e20, kernel = 'linear')\n",
    "    clf.fit(X_train, y_train)\n",
    "    svm_accuracy = np.mean(np.array(clf.predict(X_test)) == y_test)\n",
    "    avg_support_vectors += len(clf.support_vectors_)\n",
    "    \n",
    "    percentage += (int)(svm_accuracy > pla_accuracy)\n",
    "\n",
    "percentage /= num_simulations\n",
    "avg_support_vectors /= num_simulations\n",
    "print(f\"SVM is better than PLA {100 * percentage}% of the time\")\n",
    "print(\"Average number of support vectors: \", avg_support_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the code output above, the answer is **d)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 10.\n",
    "According to the code output above, the answer is **b)**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
