{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS156A Final Exam\n",
    "## Wilson Duan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1.\n",
    "Let $x = (x_1, x_2)$ be an arbitrary point in X. The linear transformation applied to $x$ results in a Z space expressed by $x_1^ix_2^j$ for all $i, j \\geq 0$ such that $i + j \\leq 10$ (except for $x_0 = 1$, which occurs when $i = j = 0$). Thus, to calculate the dimensionality of this Z space, we must count the number of $i, j$ pairs such that $i + j \\leq 10$. This is a classic counting problem:\n",
    "\n",
    "$i + j \\leq 10$\n",
    "\n",
    "$i + j + d = 10$\n",
    "\n",
    "Using sticks and stones, the number of ways to divide $i, j, d$ to sum to 10 is ${{10 + 2} \\choose 2} = {12 \\choose 2} = 66$. We then subtract one since we do not count the constant coordinate $x_0 = 1$, giving us a total of 65 $i, j$ pairs. Therefore, the dimensionality of the Z space is 65, giving us the answer **e)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2.\n",
    "The logistic regression model could result in $\\bar g \\not\\in H$. Since each hypothesis function in linear regression models comes in the form $f(x) = \\frac{e^{ax + b}}{1 + e^{ax + b}}$, if we were to add multiple hypothesis functions, there is no guarantee that the resulting sum of the functions follows the same form $\\frac{e^{ax + b}}{1 + e^{ax + b}}$, implying that $\\bar g$ may not be a logistic regression function. As a result, the answer is **d)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3.\n",
    "Statement **d)** is false because comparing $(E_{out} - E_{in})$ does not always help you determine overfitting. Solely using $(E_{out} - E_{in})$ is not a good enough metric for overfitting, as we do not know if the difference $(E_{out} - E_{in})$ can be attributed to overfitting or another factor such as noisy test data. Thus, the answer is **d)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4.\n",
    "Statement **d)** is true because stochastic noise does not depend on the hypothesis set. Stochastic noise comes solely from the data, so it has no correlation with the hypothesis set, so the answer is **d)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 5.\n",
    "Since $w_{lin}$ is the linear regression solution, we know $w_{lin}$ minimizes the mean squared error. Furthermore, since we have that $w_{lin}^T\\Gamma^T\\Gamma w_{lin} \\leq C$, $w_{lin}$ satisfies both conditions given, so we know the regularized solution is $w_{reg} = w_{lin}$. Thus, the answer is **a)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 6.\n",
    "When we use soft-order constraints to regularize polynomial models, we are minimizing mean squared error of the input data while adhering to the constraint $w^Tw \\leq C$. By choosing the right regularization term $\\lambda$, we can translate this to minimizing augmented error $E_{aug}(w) = E_{in}(w) + \\frac{\\lambda}{N}w^Tw$, so the answer is **b)**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 7.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "test_data = []\n",
    "\n",
    "with open(\"features.train\", \"r\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip().split()\n",
    "        line = [float(x) for x in line]\n",
    "        train_data.append(line)\n",
    "\n",
    "with open(\"features.test\", \"r\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip().split()\n",
    "        line = [float(x) for x in line]\n",
    "        test_data.append(line)\n",
    "\n",
    "train_data = np.array(train_data)\n",
    "test_data = np.array(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(data, k):\n",
    "    output = np.zeros((len(data), k + 1))\n",
    "    for i in range(len(data)):\n",
    "        x1, x2 = data[i]\n",
    "        output[i] = [1, x1, x2, x1*x2, x1**2, x2**2][:k + 1]\n",
    "    return output\n",
    "\n",
    "def linear_regression(X, y):\n",
    "    inversed = np.linalg.inv(X.transpose().dot(X))\n",
    "    w = inversed.dot(X.transpose()).dot(y)\n",
    "    return w\n",
    "\n",
    "def linear_regression_decay(X, y, lmbda):\n",
    "    inversed = np.linalg.inv(X.transpose().dot(X) + lmbda * np.eye(X.shape[1]))\n",
    "    w = inversed.dot(X.transpose()).dot(y)\n",
    "    return w\n",
    "\n",
    "def calculate_error(X, y, w):\n",
    "    predictions = np.sign(X.dot(w))\n",
    "    return np.mean(predictions != y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_vs_all(y, label):\n",
    "    output = []\n",
    "    for i in range(len(y)):\n",
    "        if (y[i] == label):\n",
    "            output.append(1.0)\n",
    "        else:\n",
    "            output.append(-1.0)\n",
    "    return np.array(output)\n",
    "\n",
    "def one_vs_one(X, y, label1, label2):\n",
    "    X_output = []\n",
    "    y_output = []\n",
    "    for (features, label) in zip(X, y):\n",
    "        if (label == label1):\n",
    "            X_output.append(features)\n",
    "            y_output.append(1.0)\n",
    "        elif (label == label2):\n",
    "            X_output.append(features)\n",
    "            y_output.append(-1.0)\n",
    "    return np.array(X_output), np.array(y_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E_in for 5 versus all:  0.07625840076807022\n",
      "E_in for 6 versus all:  0.09107118365107666\n",
      "E_in for 7 versus all:  0.08846523110684405\n",
      "E_in for 8 versus all:  0.07433822520916199\n",
      "E_in for 9 versus all:  0.08832807570977919\n"
     ]
    }
   ],
   "source": [
    "lmbda = 1\n",
    "X_train, y_train = train_data[:, 1:], train_data[:, 0]\n",
    "X_test, y_test = test_data[:, 1:], test_data[:, 0]\n",
    "\n",
    "# add constant to X\n",
    "X_train = transform_data(X_train, 2)\n",
    "X_test = transform_data(X_test, 2)\n",
    "\n",
    "for i in range(5, 10):\n",
    "    y_train_temp = one_vs_all(np.copy(y_train), i)\n",
    "    y_test_temp = one_vs_all(np.copy(y_test), i)\n",
    "\n",
    "    w = linear_regression_decay(X_train, y_train_temp, lmbda)\n",
    "    e_in = calculate_error(X_train, y_train_temp, w)\n",
    "    print(f'E_in for {i} versus all: ', e_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the code output above, the classifier with the lowest $E_{in}$ is 8 versus all, so the answer is **d)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E_out for 0 versus all:  0.10662680617837568\n",
      "E_out for 1 versus all:  0.02192326856003986\n",
      "E_out for 2 versus all:  0.09865470852017937\n",
      "E_out for 3 versus all:  0.08271051320378675\n",
      "E_out for 4 versus all:  0.09965122072745392\n"
     ]
    }
   ],
   "source": [
    "lmbda = 1.0\n",
    "X_train, y_train = train_data[:, 1:], train_data[:, 0]\n",
    "X_test, y_test = test_data[:, 1:], test_data[:, 0]\n",
    "\n",
    "# apply feature transform\n",
    "X_train = transform_data(X_train, 5)\n",
    "X_test = transform_data(X_test, 5)\n",
    "\n",
    "for i in range(5):\n",
    "    y_train_temp = one_vs_all(np.copy(y_train), i)\n",
    "    y_test_temp = one_vs_all(np.copy(y_test), i)\n",
    "\n",
    "    w = linear_regression_decay(X_train, y_train_temp, lmbda)\n",
    "    e_out = calculate_error(X_test, y_test_temp, w)\n",
    "    print(f'E_out for {i} versus all: ', e_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the code output above, the classifier with the lowest $E_{out}$ is 1 versus all, so the answer is **b)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E_in for 0 versus all with and without transform 0.10931285146070498 0.10231792621039638\n",
      "E_out for 0 versus all with and without transform 0.11509715994020926 0.10662680617837568\n",
      "\n",
      "E_in for 1 versus all with and without transform 0.01522424907420107 0.012343985735838706\n",
      "E_out for 1 versus all with and without transform 0.02242152466367713 0.02192326856003986\n",
      "\n",
      "E_in for 2 versus all with and without transform 0.10026059525442327 0.10026059525442327\n",
      "E_out for 2 versus all with and without transform 0.09865470852017937 0.09865470852017937\n",
      "\n",
      "E_in for 3 versus all with and without transform 0.09024825126868742 0.09024825126868742\n",
      "E_out for 3 versus all with and without transform 0.08271051320378675 0.08271051320378675\n",
      "\n",
      "E_in for 4 versus all with and without transform 0.08942531888629818 0.08942531888629818\n",
      "E_out for 4 versus all with and without transform 0.09965122072745392 0.09965122072745392\n",
      "\n",
      "E_in for 5 versus all with and without transform 0.07625840076807022 0.07625840076807022\n",
      "E_out for 5 versus all with and without transform 0.07972097658196313 0.07922272047832586\n",
      "\n",
      "E_in for 6 versus all with and without transform 0.09107118365107666 0.09107118365107666\n",
      "E_out for 6 versus all with and without transform 0.08470353761833582 0.08470353761833582\n",
      "\n",
      "E_in for 7 versus all with and without transform 0.08846523110684405 0.08846523110684405\n",
      "E_out for 7 versus all with and without transform 0.07324364723467862 0.07324364723467862\n",
      "\n",
      "E_in for 8 versus all with and without transform 0.07433822520916199 0.07433822520916199\n",
      "E_out for 8 versus all with and without transform 0.08271051320378675 0.08271051320378675\n",
      "\n",
      "E_in for 9 versus all with and without transform 0.08832807570977919 0.08832807570977919\n",
      "E_out for 9 versus all with and without transform 0.08819133034379671 0.08819133034379671\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lmbda = 1\n",
    "X_train, y_train = train_data[:, 1:], train_data[:, 0]\n",
    "X_test, y_test = test_data[:, 1:], test_data[:, 0]\n",
    "\n",
    "# add constant\n",
    "X_train_normal = transform_data(X_train, 2)\n",
    "X_test_normal = transform_data(X_test, 2)\n",
    "\n",
    "# apply feature transform\n",
    "X_train_transform = transform_data(X_train, 5)\n",
    "X_test_transform = transform_data(X_test, 5)\n",
    "\n",
    "for i in range(10):\n",
    "    y_train_temp = one_vs_all(np.copy(y_train), i)\n",
    "    y_test_temp = one_vs_all(np.copy(y_test), i)\n",
    "    \n",
    "    w_normal = linear_regression_decay(X_train_normal, y_train_temp, lmbda)\n",
    "    w_transform = linear_regression_decay(X_train_transform, y_train_temp, lmbda)\n",
    "\n",
    "    e_in_normal = calculate_error(X_train_normal, y_train_temp, w_normal)\n",
    "    e_in_transform = calculate_error(X_train_transform, y_train_temp, w_transform)\n",
    "\n",
    "    e_out_normal = calculate_error(X_test_normal, y_test_temp, w_normal)\n",
    "    e_out_transform = calculate_error(X_test_transform, y_test_temp, w_transform)\n",
    "\n",
    "    print(f'E_in for {i} versus all with and without transform', e_in_normal, e_in_transform)\n",
    "    print(f'E_out for {i} versus all with and without transform', e_out_normal, e_out_transform)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the code output above, the transform improves the out of sample performance of the 5 versus all classifier, but by less than 5%. Therefore, the answer is **e)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda = 1:\n",
      "E_in for 1 vs 5: 0.005124919923126201\n",
      "E_out for 1 vs 5: 0.025943396226415096\n",
      "lambda = 0.01:\n",
      "E_in for 1 vs 5: 0.004484304932735426\n",
      "E_out for 1 vs 5: 0.02830188679245283\n"
     ]
    }
   ],
   "source": [
    "lmbda1 = 1\n",
    "lmbda2 = 0.01\n",
    "X_train, y_train = train_data[:, 1:], train_data[:, 0]\n",
    "X_test, y_test = test_data[:, 1:], test_data[:, 0]\n",
    "\n",
    "# apply feature transform\n",
    "X_train = transform_data(X_train, 5)\n",
    "X_test = transform_data(X_test, 5)\n",
    "\n",
    "X_train, y_train = one_vs_one(X_train, y_train, 1, 5)\n",
    "X_test, y_test = one_vs_one(X_test, y_test, 1, 5)\n",
    "\n",
    "w1 = linear_regression_decay(X_train, y_train, lmbda1)\n",
    "w2 = linear_regression_decay(X_train, y_train, lmbda2)\n",
    "\n",
    "e_in1 = calculate_error(X_train, y_train, w1)\n",
    "e_in2 = calculate_error(X_train, y_train, w2)\n",
    "\n",
    "e_out1 = calculate_error(X_test, y_test, w1)\n",
    "e_out2 = calculate_error(X_test, y_test, w2)\n",
    "\n",
    "print('lambda = 1:')\n",
    "\n",
    "print(f'E_in for 1 vs 5:', e_in1)\n",
    "print(f'E_out for 1 vs 5:', e_out1)\n",
    "\n",
    "print('lambda = 0.01:')\n",
    "\n",
    "print(f'E_in for 1 vs 5:', e_in2)\n",
    "print(f'E_out for 1 vs 5:', e_out2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the code output above, when we decrease $\\lambda$ from 1 to 0.01, $E_{in}$ decreases and $E_{out}$ increases, indicating that overfitting occurs. Thus, the answer is **a)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 11."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "points = np.array([[1, 0], [0, 1], [0, -1], [-1, 0], [0, 2], [0, -2], [-2, 0]])\n",
    "labels = [-1, -1, -1, 1, 1, 1, 1]\n",
    "\n",
    "plt.scatter(points[:3, 0], points[:3, 1], c='r', label='-1')\n",
    "plt.scatter(points[3:, 0], points[3:, 1], c='b', label='+1')\n",
    "plt.legend()\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def transform_points(points):\n",
    "    output = []\n",
    "    for (x1, x2) in points:\n",
    "        z1 = x2 ** 2 - 2 * x1 - 1\n",
    "        z2 = x1 ** 2 - 2 * x2 + 1\n",
    "        output.append([z1, z2])\n",
    "    return np.array(output)\n",
    "\n",
    "z_points = transform_points(points)\n",
    "plt.scatter(z_points[:3, 0], z_points[:3, 1], c='r', label='-1')\n",
    "plt.scatter(z_points[3:, 0], z_points[3:, 1], c='b', label='+1')\n",
    "plt.legend()\n",
    "plt.xlabel(\"z1\")\n",
    "plt.ylabel(\"z2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the graph above, it is clear that the vertical line $x = 0.5$ would maximize the margin in the Z space. The weights for a vertical line are $w_1 = 1$, $w_2 = 0$, and $b = -0.5$, so the answer is **c)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 12."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "import math\n",
    "\n",
    "clf = svm.SVC(C = 10e20, kernel='poly', degree=2, coef0=1)\n",
    "clf.fit(points, labels)\n",
    "print(len(clf.support_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the code output above, the number of support vectors is 5, so the answer is **c)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 13."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a set of helper functions\n",
    "def random_point():\n",
    "    x = random.random() * 2 - 1\n",
    "    y = random.random() * 2 - 1\n",
    "    return (x, y)\n",
    "\n",
    "def f(x1, x2):\n",
    "    return np.sign(x2 - x1 + 0.25 * math.sin(math.pi * x1))\n",
    "\n",
    "def create_dataset(n):\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(n):\n",
    "        a, b = random_point()\n",
    "        X.append([a, b])\n",
    "        y.append(f(a, b))\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_data = 100\n",
    "N_simulations = 1000\n",
    "\n",
    "non_separable = 0\n",
    "for i in range(N_simulations):\n",
    "    X, y = create_dataset(N_data)\n",
    "\n",
    "    clf = svm.SVC(C = 10e20, kernel='rbf', gamma=1.5, coef0=1)\n",
    "    clf.fit(X, y)\n",
    "\n",
    "    if (clf.score(X, y) != 1):\n",
    "        non_separable += 1\n",
    "\n",
    "non_separable /= N_simulations\n",
    "non_separable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the code output above, the data is non-separable by RBF less than 5% of the time, so the answer is **a)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 14."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel(x, mu, gamma):\n",
    "    return np.exp(-gamma * np.sum((x - mu) ** 2))\n",
    "\n",
    "def get_centers(X, clusters):\n",
    "    centers = []\n",
    "    for i in range(clusters):\n",
    "        centers.append(random_point())\n",
    "    centers = np.array(centers)\n",
    "\n",
    "    prev_centers = centers.copy()\n",
    "    while (True):\n",
    "        # assign closest\n",
    "        points_by_cluster = [[] for k in range(clusters)]\n",
    "        for x in X:\n",
    "            x1, x2 = x\n",
    "            closest_cluster = -1\n",
    "            smallest_dist = 1000\n",
    "            for k in range(clusters):\n",
    "                mu1, mu2 = centers[k]\n",
    "                dist = math.sqrt((x1 - mu1) ** 2 + (x2 - mu2) ** 2)\n",
    "                if (dist < smallest_dist):\n",
    "                    smallest_dist = dist\n",
    "                    closest_cluster = k\n",
    "            points_by_cluster[closest_cluster].append(x)\n",
    "\n",
    "        # change centers to be mean\n",
    "        for k in range(clusters):\n",
    "            if (len(points_by_cluster[k]) == 0):\n",
    "                return get_centers(X, clusters)\n",
    "            cluster_points = np.array(points_by_cluster[k])\n",
    "            centers[k][0] = np.mean(cluster_points[:, 0])\n",
    "            centers[k][1] = np.mean(cluster_points[:, 1])\n",
    "\n",
    "        if ((prev_centers == centers).all()):\n",
    "            break\n",
    "        prev_centers = centers.copy()\n",
    "    return centers\n",
    "\n",
    "def get_features(X, centers, gamma):\n",
    "    output = []\n",
    "    for x in X:\n",
    "        features = [1]\n",
    "        for k in range(len(centers)):\n",
    "            features.append(kernel(x, centers[k], gamma))\n",
    "        output.append(features)\n",
    "    return np.array(output)\n",
    "\n",
    "def train_lloyds(X, y, clusters, gamma):\n",
    "    centers = get_centers(X, clusters)\n",
    "\n",
    "    X_features = get_features(X, centers, gamma)\n",
    "    w = linear_regression(X_features, y)\n",
    "    return w, centers\n",
    "\n",
    "def evaluate_lloyds(X, y, w, centers, gamma):\n",
    "    X_features = get_features(X, centers, gamma)\n",
    "    pred = np.sign(np.dot(X_features, w))\n",
    "    error = np.mean(pred != y)\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.91"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_test = 1000\n",
    "N_simulations = 100\n",
    "K = 9\n",
    "gamma = 1.5\n",
    "\n",
    "non_separable = 0\n",
    "kernel_better = 0\n",
    "for i in range(N_simulations):\n",
    "    X_train, y_train = create_dataset(N_data)\n",
    "    X_test, y_test = create_dataset(N_test)\n",
    "\n",
    "    # regular rbf\n",
    "    w, centers = train_lloyds(X_train, y_train, K, gamma)\n",
    "    e_out_lloyds = evaluate_lloyds(X_test, y_test, w, centers, gamma)\n",
    "\n",
    "    # svm\n",
    "    clf = svm.SVC(C = 10e20, kernel='rbf', gamma=gamma, coef0=1)\n",
    "    clf.fit(X_train, y_train)\n",
    "    if (clf.score(X_train, y_train) != 1):\n",
    "        non_separable += 1\n",
    "    e_out_svm = 1 - clf.score(X_test, y_test)\n",
    "    kernel_better += int(e_out_svm < e_out_lloyds)\n",
    "\n",
    "valid_simulations = N_simulations - non_separable\n",
    "kernel_better /= valid_simulations\n",
    "kernel_better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the code output above, the kernel form beats the regular form about 91% of the time, so the answer is **e)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_test = 1000\n",
    "N_simulations = 100\n",
    "K = 12\n",
    "gamma = 1.5\n",
    "\n",
    "non_separable = 0\n",
    "kernel_better = 0\n",
    "for i in range(N_simulations):\n",
    "    X_train, y_train = create_dataset(N_data)\n",
    "    X_test, y_test = create_dataset(N_test)\n",
    "\n",
    "    # regular rbf\n",
    "    w, centers = train_lloyds(X_train, y_train, K, gamma)\n",
    "    e_out_lloyds = evaluate_lloyds(X_test, y_test, w, centers, gamma)\n",
    "\n",
    "    # svm\n",
    "    clf = svm.SVC(C = 10e20, kernel='rbf', gamma=gamma, coef0=1)\n",
    "    clf.fit(X_train, y_train)\n",
    "    if (clf.score(X_train, y_train) != 1):\n",
    "        non_separable += 1\n",
    "    e_out_svm = 1 - clf.score(X_test, y_test)\n",
    "    kernel_better += int(e_out_svm < e_out_lloyds)\n",
    "\n",
    "valid_simulations = N_simulations - non_separable\n",
    "kernel_better /= valid_simulations\n",
    "kernel_better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the code output above, the kernel form beats the regular form about 81% of the time, so the answer is **d)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.61, 0.7)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_test = 1000\n",
    "N_simulations = 100\n",
    "K1 = 9\n",
    "K2 = 12\n",
    "gamma = 1.5\n",
    "\n",
    "E_in_decreased = 0\n",
    "E_out_decreased = 0\n",
    "for i in range(N_simulations):\n",
    "    X_train, y_train = create_dataset(N_data)\n",
    "    X_test, y_test = create_dataset(N_test)\n",
    "\n",
    "    w1, centers1 = train_lloyds(X_train, y_train, K1, gamma)\n",
    "    w2, centers2 = train_lloyds(X_train, y_train, K2, gamma)\n",
    "\n",
    "    # get error\n",
    "    E_in1 = evaluate_lloyds(X_train, y_train, w1, centers1, gamma)\n",
    "    E_out1 = evaluate_lloyds(X_test, y_test, w1, centers1, gamma)\n",
    "\n",
    "    E_in2 = evaluate_lloyds(X_train, y_train, w2, centers2, gamma)\n",
    "    E_out2 = evaluate_lloyds(X_test, y_test, w2, centers2, gamma)\n",
    "\n",
    "    if (E_in2 < E_in1):\n",
    "        E_in_decreased += 1\n",
    "    if (E_out2 < E_out1):\n",
    "        E_out_decreased += 1\n",
    "E_in_decreased /= N_simulations\n",
    "E_out_decreased /= N_simulations\n",
    "E_in_decreased, E_out_decreased"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the code output above, $E_{in}$ and $E_{out}$ both decrease most of the time. Thus, the answer is **d)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 17."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.25, 0.41)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_test = 1000\n",
    "N_simulations = 100\n",
    "K = 9\n",
    "gamma1 = 1.5\n",
    "gamma2 = 2\n",
    "\n",
    "E_in_decreased = 0\n",
    "E_out_decreased = 0\n",
    "for i in range(N_simulations):\n",
    "    X_train, y_train = create_dataset(N_data)\n",
    "    X_test, y_test = create_dataset(N_test)\n",
    "\n",
    "    w1, centers1 = train_lloyds(X_train, y_train, K, gamma1)\n",
    "    w2, centers2 = train_lloyds(X_train, y_train, K, gamma2)\n",
    "\n",
    "    # get error\n",
    "    E_in1 = evaluate_lloyds(X_train, y_train, w1, centers1, gamma1)\n",
    "    E_out1 = evaluate_lloyds(X_test, y_test, w1, centers1, gamma1)\n",
    "\n",
    "    E_in2 = evaluate_lloyds(X_train, y_train, w2, centers2, gamma2)\n",
    "    E_out2 = evaluate_lloyds(X_test, y_test, w2, centers2, gamma2)\n",
    "\n",
    "    if (E_in2 < E_in1):\n",
    "        E_in_decreased += 1\n",
    "    if (E_out2 < E_out1):\n",
    "        E_out_decreased += 1\n",
    "E_in_decreased /= N_simulations\n",
    "E_out_decreased /= N_simulations\n",
    "E_in_decreased, E_out_decreased"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the code output above, both $E_{in}$ and $E_{out}$ do not decrease most of the time, meaning they increase most of the time, so the answer is **c)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 18."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_test = 1000\n",
    "N_simulations = 100\n",
    "K = 9\n",
    "gamma = 1.5\n",
    "\n",
    "e_in_zero = 0\n",
    "for i in range(N_simulations):\n",
    "    X_train, y_train = create_dataset(N_data)\n",
    "    X_test, y_test = create_dataset(N_test)\n",
    "\n",
    "    # regular rbf\n",
    "    w, centers = train_lloyds(X_train, y_train, K, gamma)\n",
    "    e_in_lloyds = evaluate_lloyds(X_train, y_train, w, centers, gamma)\n",
    "    if (e_in_lloyds == 0):\n",
    "        e_in_zero += 1\n",
    "\n",
    "e_in_zero /= N_simulations\n",
    "e_in_zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the code output above, $E_{in}$ is zero 5% of the time, so the answer is **a)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 19.\n",
    "Let $I$ denote the event that our singular sample had a heart attack. By Bayes Theorem,\n",
    "\n",
    "$P(h = f \\mid I) = \\frac{P(I \\mid h = f)P(h = f)}{P(I)}$\n",
    "\n",
    "We know that $P(I)$ is a constant, so we can ignore it. We are given that $P(h = f)$ is uniform over [0, 1]. Looking at $P(I \\mid h = f)$, it is evident that as $h$ increases, the probability of a heart attack increases, so $P(I \\mid h = f)$ increases linearly. As a result, the product of $P(I \\mid h = f)$ and $P(h = f)$ results in a posterior that increases linearly over [0, 1]. Thus, the answer is **b)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 20.\n",
    "\n",
    "$E_{out}(g) = E_{out}(\\frac{1}{2}(g_1(x) + g_2(x)))$\n",
    "\n",
    "$E_{out}(g) = E[(\\frac{1}{2}(g_1(x) + g_2(x)) - y)^2]$\n",
    "\n",
    "$E_{out}(g) = \\frac{1}{4}E[(g_1(x) + g_2(x) - 2y)^2]$\n",
    "\n",
    "$E_{out}(g) = \\frac{1}{4}E[g_1(x)^2 + g_2(x)^2 - 4yg_1(x) - 4yg_2(x) + 2g_1(x)g_2(x) + 4y^2]$\n",
    "\n",
    "$E_{out}(g) = \\frac{1}{2}E[\\frac{1}{2}g_1(x)^2 + \\frac{1}{2}g_2(x)^2 - 2yg_1(x) - 2yg_2(x) + g_1(x)g_2(x) + 2y^2]$\n",
    "\n",
    "$E_{out}(g) = \\frac{1}{2}E[(g_1(x) - y)^2 + (g_2(x) - y)^2 - \\frac{1}{2}g_1(x)^2 - \\frac{1}{2}g_2(x)^2 + g_1(x)g_2(x)]$\n",
    "\n",
    "$E_{out}(g) = \\frac{E_{out}(g1) + E_{out}(g2)}{2} - \\frac{1}{2}E[\\frac{1}{2}g_1(x)^2 + \\frac{1}{2}g_2(x)^2 - g_1(x)g_2(x)]$\n",
    "\n",
    "$E_{out}(g) = \\frac{E_{out}(g1) + E_{out}(g2)}{2} - \\frac{1}{4}E[g_1(x)^2 + g_2(x)^2 - 2g_1(x)g_2(x)]$\n",
    "\n",
    "$E_{out}(g) = \\frac{E_{out}(g1) + E_{out}(g2)}{2} - \\frac{1}{4}E[(g_1(x)-g_2(x))^2]$\n",
    "\n",
    "Since $E[(g_1(x)-g_2(x))^2]$ is always greater than or equal to zero, it follows that \n",
    "\n",
    "$E_{out}(g) \\leq \\frac{E_{out}(g1) + E_{out}(g2)}{2}$\n",
    "\n",
    "Thus, the answer is **c)**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
